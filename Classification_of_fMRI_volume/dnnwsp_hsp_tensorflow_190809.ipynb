{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dnnwsp_hsp_tensorflow_190809.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"adSnE6bGuSXm","colab_type":"text"},"source":["<bn>"]},{"cell_type":"markdown","metadata":{"id":"EHUw_G_2uLvr","colab_type":"text"},"source":["# Sensorimotor experiment"]},{"cell_type":"markdown","metadata":{"id":"DElVrt4Rpgb8","colab_type":"text"},"source":["![대체 텍스트](http://cfile215.uf.daum.net/image/994FA6475D4A71B004DC66)"]},{"cell_type":"markdown","metadata":{"id":"gwjLzLq2j140","colab_type":"text"},"source":["### Task paradigm \n","- Participants conducted each of the four sensorimotor tasks including left-hand clenching (LH), right-hand clenching, auditory attention (AD), and visual stimulus (VS) tasks (Jang et al., 2017; Kim et al., 2012).\n","  - LH / RH : clenched their left or right hand approximately twice per second\n","  - AD : listened to sounds of 1 kHz during the task period and 900 Hz during the rest period\n","  - VS : watched an alternating black and white checkerboard with 8 Hz during the task period and watched a black screen during the rest period\n","- A total of 360 fMRI volumes (i.e., 30 volumes/run × 4 runs/subject × 3 subjects) were available across all tasks and all subjects.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wWXYfcoomS3W","colab_type":"text"},"source":["### Blood-oxygenation-level-dependent (BOLD) fMRI data\n","- 3-T Tim Trio MRI scanner with a 12-channel head coil (Siemens, Erlangen, Germany)\n","- A standard gradient-echo echo-planar imaging pulse sequence\n","- TR/TE = 2000/30 ms; flip angle = 90°; in-plane voxels = 64 × 64; 36 axial slices; 4 mm slice thickness without a gap; voxel size = 3.75 × 3.75 × 4.0 mm^3) \n","\n"]},{"cell_type":"markdown","metadata":{"id":"n9-h8S8Cz-8Q","colab_type":"text"},"source":["### Preprocessing of BOLD FMRI data\n","- The fMRI volumes were preprocessed using the SPM8 software toolbox (www.fil.ion.ucl.ac.uk/spm) with standard options\n","  -  including slice timing correction, motion correction, spatial normalization to the Montreal Neurological Institute space with a 3-mm isotropic voxel size, and spatial smoothing using an 8-mm full-width at half-maximum Gaussian kernel.\n"]},{"cell_type":"markdown","metadata":{"id":"-a5OFu5c1RWq","colab_type":"text"},"source":["  \n","  *Jang, H., Plis, S. M., Calhoun, V. D., & Lee, J. H. (2017). Task-specific feature extraction and classification of fMRI volumes using a deep neural network initialized with a deep belief network: Evaluation using sensorimotor tasks. Neuroimage, 145, 314-328.*\n","<bn><bn>\n","    \n","*Kim, Y. H., Kim, J., & Lee, J. H. (2012). Iterative approach of dual regression with a sparse prior enhances the performance of independent component analysis for group functional magnetic resonance imaging (fMRI) data. Neuroimage, 63(4), 1864-1889.*\n","<bn><bn>"]},{"cell_type":"markdown","metadata":{"id":"cbyR2Ud34Gsx","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"n9WoT8yd2-kV","colab_type":"text"},"source":["# Sensorimotor task classification using MLP (Tensorflow) "]},{"cell_type":"markdown","metadata":{"id":"zP2nPgHJii0D","colab_type":"text"},"source":["## Input data"]},{"cell_type":"code","metadata":{"id":"ONrIQojP9W_T","colab_type":"code","colab":{}},"source":["#!wget http://bspl.korea.ac.kr/sensorimotor.zip\n","#!unzip sensorimotor.zip\n","\n","\n","!pip install pydrive\n","from pydrive.auth import GoogleAuth \n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","data_zip = drive.CreateFile({'id':'145ZZvxYKPjRHegjNzF5gjkI7zkz5gRM7'})\n","data_zip.GetContentFile('sensorimotor.zip') \n","!unzip sensorimotor.zip\n","\n","\"\"\"\n","1. You will have to choose google account of which it will be accessing google drive.\n","2. Copy the verification code and paste it into notebook. Press enter after pasting verification code.\n","ref) https://www.javacodemonk.com/guide-to-read-data-from-google-drive-in-google-colaboratory-079a3609\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YwohfwBV1mlD","colab_type":"text"},"source":["  \n","###  Estimation of percentage of BOLD signal\n","- The BOLD intensities of the fMRI volumes within a task-related period (i.e., three blocks of 20-s task periods after a 6-s delay from the task onset for each fMRI run) were normalized to percent signal change relative to the average BOLD signal with the 80-s baseline period.\n","- A total of 360 fMRI volumes (i.e., 30 volumes/run × 4 runs/subject × 3 subjects) were available across all tasks and all subjects  \n","<bn><bn>  "]},{"cell_type":"code","metadata":{"id":"cI1DGmuWl0S2","colab_type":"code","colab":{}},"source":["tasks=['LH','RH','AD','VIS']\n","n_tasks = len(tasks)\n","n_sbj = 3\n","\n","# Installation of Nilearn to visualize nii file in Python\n","!pip install nilearn\n","from nilearn import image, plotting\n","from scipy import io as sio\n","\n","# Load data\n","image_pct_bold = [None]*n_sbj\n","task_label = [None]*n_sbj\n","for sbj in range(n_sbj):\n","    image_pct_bold[sbj] = image.load_img('sensorimotor_sbj%d.nii'%(sbj+1))\n","    task_label[sbj] = sio.loadmat('y_sensorimotor_sbj%d.mat'%(sbj+1))['y'].squeeze()\n","print('\\n ==> Data loaded to a variable')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OHV4U64lyDMv","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import numpy as np\n","\n","# Get the dimension of data\n","[xdim, ydim, zdim, tdim] = image_pct_bold[sbj].shape\n","tdim_run = int(tdim/n_tasks)\n","\n","# Examplary subject's average percentage BOLD signal\n","for run in range(n_tasks): \n","    image_plot = image.index_img(image_pct_bold[1], run*tdim_run + np.arange(tdim_run))\n","    plotting.plot_stat_map(image.mean_img(image_plot), threshold=1, title=tasks[run])\n","    plotting.show(); \n","    ;"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q8HnWhSZmqC6","colab_type":"text"},"source":["\n","### In-brain mask  \n","- The voxels whose BOLD intensities were greater than 20% of the maximum BOLD intensity for each preprocessed fMRI volume were defined as an in-brain mask from the fMRI volume. \n","- Then, the in-brain mask that overlapped across all fMRI volumes and all subjects was used to define the final set of input voxels, resulting in a total of 74,484 in-brain voxels\n"]},{"cell_type":"code","metadata":{"id":"hp8sEq7difMs","colab_type":"code","colab":{}},"source":["vMsk_3d = sio.loadmat('vMsk_3d.mat')\n","vMsk = np.where(vMsk_3d['vMsk_3d'])\n","\n","###########################################################################\n","# train_x  = 240 volumes x 74484 voxels\n","# train_y  = 240 volumes x 4 [one-hot vectors for each of LH, RH, AD and VS]\n","# test_x  = 120 volumes x 74484 voxels\n","# test_y  = 120 volumes x 4 [one-hot vectors for each of LH, RH, AD and VS]\n","###########################################################################\n","\n","# 2 subjects' vectorized functional data\n","train_x = np.hstack([image_pct_bold[0].get_data()[vMsk][:],image_pct_bold[1].get_data()[vMsk][:]]).T\n","# One-hot encoding task labels\n","train_y = np.hstack([task_label[0],task_label[1]]).T\n","train_y = np.eye(n_tasks)[train_y]\n","\n","# 1 subject's vectorized functional data\n","test_x = image_pct_bold[2].get_data()[vMsk][:].T\n","# One-hot encoding task labels\n","test_y = task_label[2]\n","test_y = np.eye(n_tasks)[test_y]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kTZ3A2K6ffm6","colab_type":"text"},"source":["## Import libraries"]},{"cell_type":"code","metadata":{"id":"UW1PZczGfd_Y","colab_type":"code","colab":{}},"source":["# -*- coding: utf-8 -*-\n","\n","# This import statement gives Python access to all of TensorFlow's classes, methods, and symbols. \n","import tensorflow as tf\n","# The fundamental package for scientific computing with Python.\n","import numpy as np\n","# Linear algebra module for calculating L1 and L2 norm  \n","from numpy import linalg as LA\n","# To plot the results\n","import matplotlib.pyplot as plt\n","# To check the directory when saving the results\n","import os.path\n","# The module for file input and output\n","import scipy.io as sio\n","# To measure time\n","import timeit\n","# To get date information\n","import datetime\n","# To get hsp combination\n","import itertools"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-G0kMQ9svKNA","colab_type":"text"},"source":["##  Define a classifier"]},{"cell_type":"markdown","metadata":{"id":"2UTHRHZokPC4","colab_type":"text"},"source":["![대체 텍스트](http://cfile236.uf.daum.net/image/998625365D4BAA2430FF08)\n","(Jang et al., 2017)"]},{"cell_type":"code","metadata":{"id":"abpJ20mlfUl8","colab_type":"code","colab":{}},"source":["\"\"\"\n","To create a MLP network  \n","\"\"\"\n","\n","def init_model(n_nodes, layer_activation):\n","    \n","    if layer_activation=='sigmoid':\n","        layer_activation = tf.nn.sigmoid\n","    elif layer_activation=='tanh':\n","        layer_activation = tf.nn.tanh\n","    elif layer_activation=='relu':\n","        layer_activation = tf.nn.relu\n","\n","\n","    # 'node_index' to split placeholder, for example, given hidden_nodes=[100, 100, 100], nodes_index=[0, 100, 200, 300]\n","    nodes_index = [int(np.sum(n_nodes[1:i+1])) for i in range(np.size(n_nodes,axis=0)-1)]\n","\n","    # Make two placeholders to fill the values later when training or testing\n","    X = tf.placeholder(tf.float32,[None,n_nodes[0]])\n","    Y = tf.placeholder(tf.float32,[None,n_nodes[-1]])\n","\n","    # Create randomly initialized weight variables\n","    w_init = [tf.math.divide(tf.random_normal([n_nodes[i],n_nodes[i+1]]), tf.sqrt(float(n_nodes[i])/2)) for i in range(np.size(n_nodes,axis=0)-1)]\n","    w = [tf.Variable(w_init[i], dtype=tf.float32) for i in range(np.size(n_nodes,axis=0)-1)]\n","    # Create zero initialized bias variables\n","    b = [tf.Variable(tf.zeros([n_nodes[i+1]]), dtype=tf.float32) for i in range(np.size(n_nodes,axis=0)-1)]\n","\n","    # Build an MLP model\n","    mlp_layers=[0.0]*(np.size(n_nodes,axis=0)-1)\n","    for i in range(np.size(n_nodes,axis=0)-1):\n","        # Input layer\n","        if i==0:\n","            mlp_layers[i] = tf.add(tf.matmul(X,w[i]),b[i])\n","            mlp_layers[i] = layer_activation(mlp_layers[i])\n","        # Hidden layers\n","        elif i>0 and i<(np.size(n_nodes,axis=0)-1):\n","            mlp_layers[i] = tf.add(tf.matmul(mlp_layers[i-1],w[i]),b[i])\n","            mlp_layers[i] = layer_activation(mlp_layers[i])\n","        # Output layer\n","        elif i==(np.size(n_nodes,axis=0)-1):\n","            mlp_layers[i] = tf.add(tf.matmul(mlp_layers[i-1],w[i]),b[i])\n","\n","\n","    return nodes_index, X, Y, mlp_layers, w, b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bTmfZ0j0w5Rz","colab_type":"text"},"source":["## Define training parameters"]},{"cell_type":"code","metadata":{"id":"Z0XX-aePw4OG","colab_type":"code","colab":{}},"source":["\"\"\"\n","L1 and L2 regularization terms\n","\n","\"\"\" \n","\n","def init_regularizations(mode, n_nodes, nodes_index, n_tg_hsp, w, b):\n","\n","    if mode=='layer':\n","        # The size is same with the number of layers\n","        Beta = tf.placeholder(tf.float32,[n_tg_hsp])\n","        # Get L1 loss term by simply multiplying beta(scalar value) and L1 norm of weight for each layer\n","        L1_loss = [tf.reduce_sum(tf.multiply(tf.abs(w[i]),Beta[i])) for i in range(n_tg_hsp)]\n","\n","    elif mode=='node':\n","        # The size is same with the number of nodes\n","        Beta = tf.placeholder(tf.float32,[np.sum(n_nodes[1:(n_tg_hsp+1)])])\n","        # Get L1 loss term by multiplying beta(vector values as many as n_nodes) and L1 norm of weight for each layer\n","        L1_loss = [tf.reduce_sum(tf.matmul(tf.abs(w[i]), tf.reshape(Beta[nodes_index[i]:nodes_index[i+1]],[-1,1]))) for i in range(n_tg_hsp)] \n","        \n","    L1_loss_total = tf.reduce_sum(L1_loss)\n","\n","    L2_REG = tf.placeholder(tf.float32)\n","    L2_loss = [tf.reduce_sum(tf.square(w[i])) for i in range(np.size(n_nodes,axis=0)-1)]\n","    L2_loss_total = tf.multiply(L2_REG,tf.reduce_sum(L2_loss))\n","\n","    return Beta, L1_loss_total, L2_REG, L2_loss_total\n","\n","\n","\"\"\" \n","Cost term (Cost = cross entropy + L1 term + L2 term ) \n","\"\"\"\n","def init_cost(mlp_layers, Y, L1_loss_total, L2_loss_total):\n","\n","    # A softmax regression : it adds up the evidence of our input being in certain classes, and converts that evidence into probabilities.\n","    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=mlp_layers[-1], labels=Y)) \\\n","                             + L1_loss_total + L2_loss_total\n","\n","    return cost\n","\n","\n","\"\"\" \n","Error term  \n","\"\"\"\n","def init_error(mlp_layers, Y):\n","\n","    correct_prediction = tf.equal(tf.argmax(mlp_layers[-1],1),tf.argmax(Y,1))\n","    error = 1-tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n","\n","    \n","    return error\n","\n","\n","\"\"\"\n","TensorFlow provides optimizers that slowly change each variable in order to minimize the loss function.\n","\"\"\"\n","def init_optimizer(optimizer_algorithm, cost, momentum):\n","    \n","    Lr = tf.placeholder(tf.float32)\n","    \n","    if optimizer_algorithm=='GradientDescent':\n","        optimizer = tf.train.GradientDescentOptimizer(Lr).minimize(cost)\n","    elif optimizer_algorithm=='Adagrad':\n","        optimizer = tf.train.AdagradOptimizer(Lr).minimize(cost)\n","    elif optimizer_algorithm=='Adam':\n","        optimizer = tf.train.AdamOptimizer(Lr).minimize(cost)\n","    elif optimizer_algorithm=='Momentum':\n","        optimizer = tf.train.MomentumOptimizer(Lr,momentum).minimize(cost)\n","    elif optimizer_algorithm=='RMSProp':\n","        optimizer = tf.train.RMSPropOptimizer(Lr).minimize(cost)\n","\n","    return Lr, optimizer\n","\n","\n","\n","\"\"\"\n","Variables to store training results\n","\"\"\"\n","def init_savingVariables(mode, n_tg_hsp):\n","    if mode=='layer':\n","        beta_val = np.zeros(n_tg_hsp)\n","        beta_vec = np.zeros(n_tg_hsp)\n","        hsp_val = np.zeros(n_tg_hsp)\n","        result_beta = np.zeros(n_tg_hsp)\n","        result_hsp = np.zeros(n_tg_hsp)\n","\n","    elif mode=='node':\n","        beta_val = [np.zeros(n_nodes[i+1]) for i in range(n_tg_hsp)]\n","        beta_vec = np.zeros(np.sum(n_nodes[1:-1]))\n","        hsp_val = [np.zeros(n_nodes[i+1]) for i in range(n_tg_hsp)]\n","        result_beta = [np.zeros(n_nodes[i+1]) for i in range(n_tg_hsp)]\n","        result_hsp = [np.zeros(n_nodes[i+1]) for i in range(n_tg_hsp)]\n","\n","\n","    # make arrays to store and plot results\n","    result_lr = np.zeros(1)\n","    result_cost = np.zeros(1)\n","    result_train_err = np.zeros(1)\n","    result_test_err = np.zeros(1)\n","\n","    return beta_val, beta_vec, hsp_val, result_beta, result_hsp, result_lr, result_cost, result_train_err, result_test_err\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2au0JvzFxAyA","colab_type":"text"},"source":["## Define Hoyer's sparsity control function"]},{"cell_type":"markdown","metadata":{"id":"aNhRfYgOjfrf","colab_type":"text"},"source":["![대체 텍스트](http://cfile211.uf.daum.net/image/99936D3C5D4BA942271035)"]},{"cell_type":"code","metadata":{"id":"GmOwaZUow1-h","colab_type":"code","colab":{}},"source":["\"\"\"\n","Hoyer's sparsity level control\n","\"\"\"\n","\n","def Hoyers_sparsity_control(mode, beta_lrates, W, b, max_b, tg):\n","\n","    # Weight sparsity control with Hoyer's sparsness (Layer wise)\n","    if mode=='layer':\n","\n","        # Get value of weight\n","        [n_nodes,dim]=W.shape\n","        num_elements=n_nodes*dim\n","\n","        Wvec=W.flatten()\n","\n","        # Calculate L1 and L2 norm\n","        L1=LA.norm(Wvec,1)\n","        L2=LA.norm(Wvec,2)\n","\n","        # Calculate hoyer's sparsness\n","        h=(np.sqrt(num_elements)-(L1/L2))/(np.sqrt(num_elements)-1)\n","\n","        # Update beta\n","        b-=beta_lrates*np.sign(h-tg)\n","\n","        # Trim value\n","        b=0.0 if b<0.0 else b\n","        b=max_b if b>max_b else b\n","\n","        return [h,b]\n","\n","    # Weight sparsity control with Hoyer's sparsness (Node wise)\n","    elif mode=='node':\n","\n","        b_vec = b\n","\n","        # Get value of weight\n","        [n_nodes,dim]=W.shape\n","\n","        # Calculate L1 and L2 norm\n","        L1=LA.norm(W,1,axis=0)\n","        L2=LA.norm(W,2,axis=0)\n","\n","        h_vec = np.zeros((1,dim))\n","        tg_vec = np.ones(dim)*tg\n","\n","        # Calculate hoyer's sparsness\n","        h_vec=(np.sqrt(n_nodes)-(L1/L2))/(np.sqrt(n_nodes)-1)\n","\n","        # Update beta\n","        b_vec-=beta_lrates*np.sign(h_vec-tg_vec)\n","\n","        # Trim value\n","        b_vec[b_vec<0.0]=0.0\n","        b_vec[b_vec>max_b]=max_b\n","\n","\n","        return [h_vec,b_vec]\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t4m5cyX1gV6a","colab_type":"text"},"source":["## Set hyperparameters for training"]},{"cell_type":"code","metadata":{"id":"3xQnj4lofoOH","colab_type":"code","colab":{}},"source":["\"\"\" \n","Set the number of nodes for input, hidden layers and output layer\n","\"\"\"\n","n_nodes=[74484,100,100,100,4]\n","\n","\n","\n","\"\"\"\n","Select optimizer :\n","    'GradientDescent'\n","    'Adagrad'\n","    'Adam'\n","    'Momentum'\n","    'RMSProp'\n","\"\"\"\n","optimizer_algorithm = 'Momentum'\n","\n","momentum = 0.5\n","\n","\n","\"\"\"\n","Set learning hyperparameters\n","\"\"\"\n","\n","# Total epochs\n","n_epochs = 250\n","# Mini batch size\n","batch_size = 30\n","# Initial learning rate\n","lr_init = 0.002\n","# Learning rate anealing after **th epoch\n","begin_anneal = 100\n","# Learning rate decay rate\n","decay_rate = 0.001\n","# Minimum learning rate\n","lr_min = 0.0002\n","\n","# Layer activation 'sigmoid', 'tanh', 'relu'\n","layer_activation = 'tanh'\n","\n","# Lambda for L2 regularization\n","L2_reg = 1e-4\n","\n","\n","\"\"\"\n","Select the sparsity control mode\n","'layer' for layer wise sparsity control\n","'node' for node wise sparsity control\n","\"\"\"\n","\n","mode = 'layer'\n","\n","# Max beta\n","max_beta = [0.01, 0.2, 0.2] #[0.02, 0.5, 0.5]\n","# Beta learning rate\n","beta_lr = [0.002, 0.05, 0.05]\n","\n","# Candidates of target sparsness level for each layer (0:dense~1:sparse)\n","hsp_cands = list(itertools.product([0.8, 0.6],[0.8, 0.6],[0.8, 0.6])) #[[0.8, 0.8, 0.8]]\n","n_hsp_cands = len(hsp_cands)\n","\n","\n","\"\"\"\n","Set the directory to save results\n","\"\"\"\n","directory_path = os.getcwd() #'/users/hailey/code/03_code/Tensorflow_tutorial/dnnwsp-master/Tensorflow_code'\n","\n","# make a new 'results' directory in the current directory\n","dtime = datetime.datetime.now()\n","directory_save = os.path.join(directory_path, r'results_%04d%02d%02d_%02d%02d'%(dtime.year,dtime.month,dtime.day,dtime.hour,dtime.minute))\n","if not os.path.exists(directory_save):\n","    os.makedirs(directory_save)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vWj0YtPLE1Df","colab_type":"text"},"source":["## Create a model using definitions above"]},{"cell_type":"code","metadata":{"id":"vA_yKqzTDDyh","colab_type":"code","colab":{}},"source":["nodes_index, X, Y, mlp_layers, w, b = init_model(n_nodes, layer_activation)\n","\n","Beta, L1_loss_total, L2_REG, L2_loss_total = init_regularizations(mode, n_nodes, nodes_index, len(hsp_cands[0]), w, b)\n","cost = init_cost(mlp_layers, Y, L1_loss_total, L2_loss_total)\n","error = init_error(mlp_layers, Y)\n","\n","Lr, optimizer = init_optimizer(optimizer_algorithm, cost, momentum)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-nR8Wp_KxqTU","colab_type":"text"},"source":["## Learning"]},{"cell_type":"code","metadata":{"id":"FvhWKKrgxFor","colab_type":"code","colab":{}},"source":["start_time = timeit.default_timer()\n","\n","error_vd = [None]*n_hsp_cands\n","hsp_vd = [None]*n_hsp_cands\n","\n","for i_tg in range(n_hsp_cands+1):\n","    \n","        \n","    ### In case of a validation phase \n","    if i_tg < n_hsp_cands:      \n","        vd_ts = 'Validation'\n","        # One of candidates\n","        hsp_tg = hsp_cands[i_tg]\n","        n_hsp_tg = len(hsp_tg)\n","        \n","        # One subject from train data as inner training set\n","        train_x_set = train_x[:int(np.size(train_x,axis=0)/2)][:]\n","        train_y_set = train_y[:int(np.size(train_y,axis=0)/2)][:]\n","        # The other subject from train data as inner validation set\n","        test_x_set = train_x[int(np.size(train_x,axis=0)/2):][:]\n","        test_y_set = train_y[int(np.size(train_y,axis=0)/2):][:]\n","        print(\"\\n ****************************** Validation phase *******************************\")\n","        print(\"# (\",i_tg+1,\"/\",n_hsp_cands,\") hsp candidate :\",hsp_tg,\"#\")\n","        \n","    ### In case of a test phase\n","    else:\n","        vd_ts = 'Test'\n","        # To set a target as an optimal sparsity level\n","        i_selected = np.argmin(error_vd)      \n","        hsp_tg = hsp_cands[i_selected]\n","        n_hsp_tg = len(hsp_tg) \n","\n","        train_x_set = train_x[:][:]\n","        train_y_set = train_y[:][:]\n","        test_x_set = test_x[:][:]\n","        test_y_set = test_y[:][:]\n","        print(\"\\n ****************************** Test phase *******************************\")\n","        print(\"############### ==> Selected hsp :\",hsp_tg,\"###############\")\n","\n","\n","        \n","        \n","    # Training starts    \n","    with tf.Session(config=tf.ConfigProto(log_device_placement=False)) as sess:\n","        \n","        beta_val, beta_vec, hsp_val, result_beta, result_hsp, result_lr, result_cost, result_train_err, result_test_err = init_savingVariables(mode, n_hsp_tg)\n","\n","        \n","        # To initialize all the variables in a TensorFlow program, you must explicitly call a special operation\n","        init = tf.global_variables_initializer()\n","        sess.run(init)\n","\n","\n","        # Start training\n","        for epoch in range(n_epochs):\n","\n","            # Shuffle training data at the begining of each epoch\n","            n_samples = np.size(train_x_set, axis=0)\n","            shuff_ids = np.arange(n_samples)\n","            np.random.shuffle(shuff_ids)\n","\n","\n","            # Begin annealing\n","            if epoch == 0:\n","                lr = lr_init\n","            elif decay_rate!=0 and (epoch+1)>begin_anneal:\n","                lr = max( lr_min, (-decay_rate*(epoch+1) + (1+decay_rate*begin_anneal)) * lr )\n","\n","\n","            \n","            cost_epoch=0.0\n","            \n","            # How many mini-batch iterations we need\n","            n_batches = int(np.ceil(n_samples/batch_size))\n","            \n","            # minibatch based training\n","            for batch in range(n_batches):\n","                first_sample = batch*batch_size\n","                last_sample = min((batch+1)*batch_size, n_samples)\n","                batch_x = train_x_set[shuff_ids[first_sample:last_sample]]\n","                batch_y = train_y_set[shuff_ids[first_sample:last_sample]]\n","  \n","                # Get cost and optimize the model\n","                cost_batch,_ = sess.run([cost,optimizer],{Lr:lr, X:batch_x, Y:batch_y, Beta:beta_vec, L2_REG:L2_reg})\n","                cost_epoch += cost_batch/n_batches\n","\n","                # Weight sparsity control\n","                if mode=='layer':\n","                    for i in range(n_hsp_tg):\n","                        [hsp_val[i], beta_val[i]] = Hoyers_sparsity_control(mode, beta_lr[i], sess.run(w[i]), beta_val[i], max_beta[i], hsp_tg[i])\n","                    beta_vec = beta_val[:]\n","\n","                elif mode=='node':\n","                    for i in range(n_hsp_tg):\n","                        [hsp_val[i], beta_val[i]] = Hoyers_sparsity_control(mode, beta_lr[i], sess.run(w[i]), beta_val[i], max_beta[i], hsp_tg[i])\n","                    # flatten beta_val (shape (3, 100) -> (300,))\n","                    beta_vec = [item for sublist in beta_val for item in sublist]\n","\n","            # get train and test error\n","            train_err_epoch = sess.run(error,{X:train_x_set, Y:train_y_set}) \n","            test_err_epoch = sess.run(error,{X:test_x_set, Y:test_y_set})\n","            \n","\n","            # Save the results to array\n","            result_train_err = np.hstack([result_train_err,[train_err_epoch]])\n","            result_test_err = np.hstack([result_test_err,[test_err_epoch]])\n","            result_lr = np.hstack([result_lr,[lr]])\n","            result_cost = np.hstack([result_cost,[cost_epoch]])\n","\n","            if mode=='layer':\n","                result_hsp=[np.vstack([result_hsp[i],[hsp_val[i]]]) for i in range(n_hsp_tg)]\n","                result_beta=[np.vstack([result_beta[i],[beta_val[i]]]) for i in range(n_hsp_tg)]\n","\n","            elif mode=='node':\n","                result_hsp=[np.vstack([result_hsp[i],[np.transpose(hsp_val[i])]]) for i in range(n_hsp_tg)]\n","                result_beta=[np.vstack([result_beta[i],[np.transpose(beta_val[i])]]) for i in range(n_hsp_tg)]\n","\n","            # Print result at epoch \n","            if ((epoch+1)%50==0):             \n","                print(\"< Epoch\", \"{:02d}\".format(epoch+1),\"> Cost :\", \"{:.4f}\".format(cost_epoch)\\\n","                                                ,\"/ Train err :\", \"{:.4f}\".format(train_err_epoch),\"/ \",vd_ts,\" err :\",\"{:.4f}\".format(test_err_epoch))\n","                print(\"             beta :\",np.mean(np.swapaxes(result_beta,0,1)[-1],axis=1),\"/ hsp :\",np.mean(np.swapaxes(result_hsp,0,1)[-1],axis=1))\n","\n","                \n","        print(\"\")\n","        print(\"-> Accuracy :\", \"{:.3f}\".format(1-result_test_err[-1]))\n","        \n","        \n","            \n","        # Plot and save the figures\n","        \n","        # Make a new results directory for the target\n","        hsp_tg_str=['0.%02d-'%(i*100) for i in hsp_tg]\n","        hsp_tg_str[-1]=hsp_tg_str[-1][:-1]\n","        hsp_tg_str=''.join(hsp_tg_str)\n","\n","        directory_results = os.path.join(directory_save, r'%s_hsp_tg_%s'%(vd_ts,hsp_tg_str))\n","        if not os.path.exists(directory_results):\n","            os.makedirs(directory_results)\n","\n","\n","        # Plot the change of learning rate\n","        plt.figure()\n","        plt.title(\"Learning rate plot\",fontsize=16)\n","        result_lr=result_lr[1:]\n","        plt.ylim(0.0, lr_init*1.2)\n","        plt.plot(result_lr)\n","        plt.savefig(directory_results+'/learning_rate.png')\n","        plt.show(block=False)\n","\n","\n","        # Plot the change of cost\n","        plt.figure()\n","        plt.title(\"Cost plot\",fontsize=16)\n","        result_cost=result_cost[1:]\n","        plt.plot(result_cost)\n","        plt.savefig(directory_results+'/cost.png')\n","        plt.show(block=False)\n","\n","\n","        # Plot train & test error\n","        plt.figure()\n","        plt.title(\"Train & Test error plot\",fontsize=16)\n","        result_train_err=result_train_err[1:]\n","        plt.plot(result_train_err, label='Train error')\n","        result_test_err=result_test_err[1:]\n","        plt.plot(result_test_err, label='%s error'%vd_ts)\n","        plt.ylim(0.0, 1.0)\n","        plt.legend(loc='upper right')\n","        plt.show(block=False)\n","        plt.savefig(directory_results+'/error_'+\"tr{:.4f}_ts{:.4f}\".format(result_train_err[-1],result_test_err[-1])+'.png')\n","\n","\n","        # Plot the change of beta value\n","        print(\"\")\n","        plt.figure()\n","        for i in range(n_hsp_tg):\n","            print(\"\")\n","            plt.title(\"Beta plot\",fontsize=16)\n","            result_beta[i]=result_beta[i][1:]\n","            plt.plot(result_beta[i], label='layer%d'%(i+1) if mode=='layer' else '')\n","            plt.ylim(0.0, np.max(max_beta)*1.2)\n","        plt.legend(loc='best')\n","        plt.show(block=False)\n","        plt.savefig(directory_results+'/beta.png')\n","\n","\n","        # Plot the change of Hoyer's sparsity level\n","        print(\"\")\n","        plt.figure()\n","        for i in range(n_hsp_tg):\n","            print(\"\")\n","            plt.title(\"Hoyer's sparsity plot\",fontsize=16)\n","            result_hsp[i]=result_hsp[i][1:]\n","            plt.plot(result_hsp[i], label='layer%d'%(i+1) if mode=='layer' else '')\n","            plt.ylim(0.0, 1.0)\n","        plt.legend(loc='best')\n","        plt.show(block=False)\n","        if n_hsp_tg == 3:\n","            plt.savefig(directory_results + '/hsp_final_' + \"{:.3f}\".format(\n","                np.mean(result_hsp[0][-1])) + '_' + \"{:.3f}\".format(\n","                np.mean(result_hsp[1][-1])) + '_' + \"{:.3f}\".format(\n","                np.mean(result_hsp[2][-1])) + '.png')\n","        elif n_hsp_tg == 2:\n","            plt.savefig(directory_results + '/hsp_final_' + \"{:.3f}\".format(\n","                np.mean(result_hsp[0][-1])) + '_' + \"{:.3f}\".format(\n","                np.mean(result_hsp[1][-1])) + '.png')\n","        elif n_hsp_tg == 1:\n","            plt.savefig(directory_results + '/hsp_final_' + \"{:.3f}\".format(\n","                np.mean(result_hsp[0][-1])) + '.png')\n","            \n","            \n","        # save results as .mat file\n","        sio.savemat(directory_results+\"/result_learningrate.mat\", mdict={'lr': result_lr})\n","        sio.savemat(directory_results+\"/result_cost.mat\", mdict={'cost': result_cost})\n","        sio.savemat(directory_results+\"/result_train_err.mat\", mdict={'trainErr': result_train_err})\n","        sio.savemat(directory_results+\"/result_test_err.mat\", mdict={'testErr': result_test_err})\n","        sio.savemat(directory_results+\"/result_beta.mat\", mdict={'beta': result_beta})\n","        sio.savemat(directory_results+\"/result_hsp.mat\", mdict={'hsp': result_hsp})\n","        sio.savemat(directory_results+\"/result_weight.mat\", mdict={'weight': sess.run(w)})\n","        sio.savemat(directory_results+\"/result_bias.mat\", mdict={'bias': sess.run(b)})\n","        \n","        # If it was validation phase\n","        if i_tg < n_hsp_cands: \n","            plt.close('all')\n","            error_vd[i_tg] = result_test_err[-1]\n","            hsp_vd[i_tg] = np.mean(np.swapaxes(result_hsp,0,1)[-1],axis=1)\n","            \n","            \n","        \n","end_time = timeit.default_timer()\n","\n","print('\\nThe code ran for %d mins' %((end_time-start_time)/60)+'\\n\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lb2yazSSxGBp","colab_type":"text"},"source":["## Save parameters and summary as text file"]},{"cell_type":"code","metadata":{"id":"-GtHQpYlxJmX","colab_type":"code","colab":{}},"source":["f = open(directory_save+\"/parameters_summary.txt\",'w') \n","f.write('mode : '+str(mode)+'\\n')\n","f.write('optimizer_algorithm : '+str(optimizer_algorithm)+'\\n')\n","f.write('layer_activation : '+str(layer_activation)+'\\n')\n","f.write('n_epochs : '+str(n_epochs)+'\\n')\n","f.write('batch_size : '+str(batch_size)+'\\n')\n","f.write('lr_init : '+str(lr_init)+'\\n')\n","f.write('lr_min : '+str(lr_min)+'\\n')\n","f.write('begin_anneal : '+str(begin_anneal)+'\\n')\n","f.write('decay_rate : '+str(decay_rate)+'\\n')\n","f.write('L2_reg : '+str(L2_reg)+'\\n')\n","f.write('max_beta : '+str(max_beta)+'\\n')\n","f.write('beta_lr : '+str(beta_lr)+'\\n')\n","f.write('---\\n')\n","for i_tg in range(n_hsp_cands):\n","    f.write('('+str(i_tg+1)+'/'+str(n_hsp_cands)+') tg :'+str(hsp_cands[i_tg])+'/ hsp : '+str(hsp_vd[i_tg])+'- val error : '+\"{:.4f}\".format(error_vd[i_tg])+'\\n')\n","f.write('==> Selected tg :'+str(hsp_cands[i_selected])+'/ hsp : '+str(np.mean(np.swapaxes(result_hsp,0,1)[-1],axis=1))+'- ts error : '+\"{:.4f}\".format(result_test_err[-1])+'\\n')\n","f.write('\\n The code ran for %d mins' %((end_time-start_time)/60)+'\\n\\n')\n","f.close()\n"],"execution_count":0,"outputs":[]}]}